{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c804838d-56b3-47ef-83a7-68d366bb8f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Appercu-------\n",
      "  Password  strength\n",
      "0    7hqwv         0\n",
      "1     cjml         0\n",
      "2     asuy         0\n",
      "3    kcyth         0\n",
      "4     whcq         0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df_very_weak = pd.read_csv(\"D:/Etudes/2CS/Stage/PWLDS-main/pwlds_very_weak.csv\")\n",
    "df_weak = pd.read_csv(\"D:/Etudes/2CS/Stage/PWLDS-main/pwlds_weak.csv\")\n",
    "df_average = pd.read_csv(\"D:/Etudes/2CS/Stage/PWLDS-main/pwlds_average.csv\")\n",
    "df_strong = pd.read_csv(\"D:/Etudes/2CS/Stage/PWLDS-main/pwlds_strong.csv\")\n",
    "\n",
    "dataB = pd.concat([\n",
    "    df_very_weak,\n",
    "    df_weak,\n",
    "    df_average,\n",
    "    df_strong,\n",
    "], ignore_index=True)\n",
    "dataB = dataB.rename(columns={\"Strength_Level\": \"strength\"})\n",
    "\n",
    "print(\"----------Appercu-------\")\n",
    "print(dataB.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e38d3e-3a71-456e-9969-a7bf784e563f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Nombre de donnees--------\n",
      "strength\n",
      "3    2000382\n",
      "0    2000043\n",
      "2    2000024\n",
      "1    2000021\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"---------Nombre de donnees--------\")\n",
    "print(dataB[\"strength\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376292ef-4e4b-4391-b9e4-06c1b0ecd6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille après équilibrage : (200000, 2)\n",
      "----------Informations-------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   Password  200000 non-null  object\n",
      " 1   strength  200000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "min_class_size = 50000\n",
    "\n",
    "# Indices équilibrés\n",
    "balanced_indices = []\n",
    "\n",
    "for cls in dataB[\"strength\"].unique():\n",
    "    cls_indices = dataB[dataB[\"strength\"] == cls].index\n",
    "    sampled = np.random.choice(cls_indices, size=min_class_size, replace=False)\n",
    "    balanced_indices.extend(sampled)\n",
    "\n",
    "# Créer dataset équilibré\n",
    "data = dataB.loc[balanced_indices].reset_index(drop=True)\n",
    "\n",
    "print(\"Taille après équilibrage :\", data.shape)\n",
    "print(\"----------Informations-------\")\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b5ff8bf-1726-4b17-8fd3-bbdcf7520271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les nuls s il y en a\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a67f7ab-79d5-49af-9091-0179baaaa49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LogisticRegression ===\n",
      "Accuracy: 0.92705\n",
      "F1-macro: 0.9274101714452265\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     10001\n",
      "           1       0.99      0.97      0.98      9992\n",
      "           2       0.83      0.90      0.87      9916\n",
      "           3       0.89      0.84      0.86     10091\n",
      "\n",
      "    accuracy                           0.93     40000\n",
      "   macro avg       0.93      0.93      0.93     40000\n",
      "weighted avg       0.93      0.93      0.93     40000\n",
      "\n",
      "\n",
      "=== Linear SVM ===\n",
      "Accuracy: 0.928375\n",
      "F1-macro: 0.928881640464907\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     10001\n",
      "           1       0.99      0.96      0.97      9992\n",
      "           2       0.84      0.90      0.87      9916\n",
      "           3       0.89      0.86      0.87     10091\n",
      "\n",
      "    accuracy                           0.93     40000\n",
      "   macro avg       0.93      0.93      0.93     40000\n",
      "weighted avg       0.93      0.93      0.93     40000\n",
      "\n",
      "\n",
      "=== RandomForest ===\n",
      "Accuracy: 0.779175\n",
      "F1-macro: 0.7837736165526561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     10001\n",
      "           1       0.99      0.74      0.85      9992\n",
      "           2       0.65      0.59      0.62      9916\n",
      "           3       0.59      0.78      0.67     10091\n",
      "\n",
      "    accuracy                           0.78     40000\n",
      "   macro avg       0.80      0.78      0.78     40000\n",
      "weighted avg       0.80      0.78      0.78     40000\n",
      "\n",
      "\n",
      "=== XGBoost ===\n",
      "Accuracy: 0.935025\n",
      "F1-macro: 0.9348537178964313\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     10001\n",
      "           1       0.98      0.98      0.98      9992\n",
      "           2       0.89      0.85      0.87      9916\n",
      "           3       0.87      0.91      0.89     10091\n",
      "\n",
      "    accuracy                           0.94     40000\n",
      "   macro avg       0.94      0.93      0.93     40000\n",
      "weighted avg       0.94      0.94      0.93     40000\n",
      "\n",
      "\n",
      "=== Decision Tree ===\n",
      "Accuracy: 0.75735\n",
      "F1-macro: 0.7533126833713174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     10001\n",
      "           1       0.66      0.74      0.69      9992\n",
      "           2       0.81      0.50      0.62      9916\n",
      "           3       0.63      0.79      0.70     10091\n",
      "\n",
      "    accuracy                           0.76     40000\n",
      "   macro avg       0.77      0.76      0.75     40000\n",
      "weighted avg       0.77      0.76      0.75     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choix de l algorithme\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "# ============================\n",
    "# Préparation des données\n",
    "# ============================\n",
    "X = data[\"Password\"]\n",
    "y = data[\"strength\"]\n",
    "\n",
    "# Fonction lenTransform : calcule la longueur\n",
    "def lenTransform(passwords):\n",
    "    return np.array([len(pw) for pw in passwords]).reshape(-1, 1)\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF (n-grams caractères)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer=\"char\",\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=2000\n",
    ")\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Longueur des mots de passe\n",
    "X_train_len = lenTransform(X_train)\n",
    "X_test_len = lenTransform(X_test)\n",
    "\n",
    "# Normalisation de la longueur\n",
    "scaler = StandardScaler()\n",
    "X_train_len_scaled = scaler.fit_transform(X_train_len)\n",
    "X_test_len_scaled = scaler.transform(X_test_len)\n",
    "\n",
    "# Combinaison TF-IDF + feature \"len\"\n",
    "X_train_combined = hstack([X_train_tfidf, csr_matrix(X_train_len_scaled)])\n",
    "X_test_combined = hstack([X_test_tfidf, csr_matrix(X_test_len_scaled)])\n",
    "\n",
    "# ============================\n",
    "# Tests des modèles\n",
    "# ============================\n",
    "\n",
    "# ===== Logistic Regression =====\n",
    "print(\"=== LogisticRegression ===\")\n",
    "clf_lr = LogisticRegression(max_iter=1000, n_jobs=-1, class_weight=\"balanced\")\n",
    "clf_lr.fit(X_train_combined, y_train)\n",
    "y_pred_lr = clf_lr.predict(X_test_combined)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"F1-macro:\", f1_score(y_test, y_pred_lr, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# ===== Linear SVM =====\n",
    "print(\"\\n=== Linear SVM ===\")\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train_combined, y_train)\n",
    "y_pred_svm = svm.predict(X_test_combined)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"F1-macro:\", f1_score(y_test, y_pred_svm, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "\n",
    "\n",
    "# ===== Random Forest =====\n",
    "print(\"\\n=== RandomForest ===\")\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, class_weight=\"balanced\",max_depth=10, random_state=42)\n",
    "rf.fit(X_train_combined, y_train)\n",
    "y_pred_rf = rf.predict(X_test_combined)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"F1-macro:\", f1_score(y_test, y_pred_rf, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# ===== XGBoost =====\n",
    "print(\"\\n=== XGBoost ===\")\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "xgb.fit(X_train_combined, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test_combined)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"F1-macro:\", f1_score(y_test, y_pred_xgb, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# ===== Decision Tree =====\n",
    "print(\"\\n=== Decision Tree ===\")\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=20,      \n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "dt.fit(X_train_combined, y_train)\n",
    "y_pred_dt = dt.predict(X_test_combined)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"F1-macro:\", f1_score(y_test, y_pred_dt, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f1d87d0-2452-47a1-b318-6f0472579bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_len.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Sauvegarder Logistic Regression\n",
    "joblib.dump(clf_lr, \"logistic_password_model.pkl\")\n",
    "\n",
    "# Sauvegarder TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Sauvegarder scaler\n",
    "joblib.dump(scaler, \"scaler_len.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf771330-4d21-4a7e-9e9a-f2a625f1c475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat : [0 2 3 3 2]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import joblib\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Charger le modèle, vectorizer et scaler\n",
    "clf_lr = joblib.load(\"logistic_password_model.pkl\")\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "scaler = joblib.load(\"scaler_len.pkl\")\n",
    "\n",
    "# Exemple de nouveaux mots de passe\n",
    "new_passwords = [\"12345\", \"AcinetaeU\", \"ChevalTrottinetteAbricotMaison\", \"!L5uraA1meLesAffr3uxBurgers\",\"abstemiousy&\"]\n",
    "\n",
    "# TF-IDF\n",
    "X_new_tfidf = vectorizer.transform(new_passwords)\n",
    "\n",
    "# Longueur des mots de passe\n",
    "X_new_len = np.array([len(pw) for pw in new_passwords]).reshape(-1, 1)\n",
    "\n",
    "# Standardisation\n",
    "X_new_len_scaled = scaler.transform(X_new_len)\n",
    "\n",
    "# Combinaison\n",
    "X_new_combined = hstack([X_new_tfidf, csr_matrix(X_new_len_scaled)])\n",
    "\n",
    "# Prédiction\n",
    "y_pred_new = clf_lr.predict(X_new_combined)\n",
    "print(\"Résultat :\", y_pred_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3021a6b6-b90d-45ab-9ba5-f03789a2e082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
